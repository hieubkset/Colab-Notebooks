{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "overfit_and_underfit.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieubkset/Colab-Notebooks/blob/master/overfit_and_underfit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C9HmC2T4ld5B"
      },
      "source": [
        "# **Overfit and underfit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "19rPukKZsPG6"
      },
      "source": [
        "Ở hai bài trước  [phân loại văn bản](https://ezcodin.com/tf03) và [regression](https://ezcodin.com/tf05), chúng ta đã thấy accuracy trên tập validation đạt đỉnh sau một vài epoch, sau đó nếu tiếp tục training nó sẽ bão hòa hoặc giảm.\n",
        "\n",
        "\n",
        "Hiện tượng này được gọi là overfitting. Mặc dù overfitting có thể dẫn tới accuracy cao trên tập training nhưng mục tiêu của chúng ta là accuracy trên tập test (hay accuracy của model khi áp dụng vào thực tế).\n",
        "\n",
        "Ngược lại với overfitting là underfitting. Nó là hiện tượng model có accuracy thấp trên cả tập train và tập test. Nguyên nhân có thể là: model quá đơn giản, bị regularize quá mức, hoặc là không đủ data cho training. \n",
        "\n",
        "Các kỹ thuật phổ biến để tránh overfitting bao gồm:\n",
        "\n",
        "+ Thu thập thên data.\n",
        "+ Data augmentation.\n",
        "+ Giảm capacity .\n",
        "+ Sử dụng weight regularization.\n",
        "+ Sử dụng dropout.\n",
        "\n",
        "Trong bài này, chúng ta sẽ tìm hiểu ảnh hưởng của capacity đến overfitting, cách sử dụng L2 regularization và dropout.\n",
        "\n",
        "Mục tiêu:\n",
        "\n",
        "+ Làm quen với kỹ thuật adaptive learning rate.\n",
        "+ Học cách áp dụng một biến đối lên tất cả các phần tử trong dataset. Kỹ thuật này có thể ứng dụng trong nhiều trường hợp, ví dụ: data augumentation. Trong bài này, ta sẽ ứng dụng để tái cấu trúc dữ liệu của các phần tử.\n",
        "+ Ôn tập kỹ thuật Early Stopping.\n",
        "+ Tìm hiểu ảnh hưởng capacity.\n",
        "+ Học cách sử dụng weight regularization và dropout.\n",
        "+ Làm quen với cách sử dụng Tensorboard. Một công cụ rất hiệu quả để đánh giá overfitting và underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4GK-mEom1QJ",
        "colab_type": "text"
      },
      "source": [
        "## Chương trình"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WL8UoOTmGGsL"
      },
      "source": [
        "#### **1. Khai báo các thư viện**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrE07xcNuVSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pZ8A2liqvgk",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-pnOU-ctX27Q",
        "colab": {}
      },
      "source": [
        "from  IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import shutil\n",
        "import tempfile\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QBPxDa1unHC",
        "colab_type": "text"
      },
      "source": [
        "Chúng ta sẽ sử dụng một số API từ [Tensorflow Docs](https://github.com/tensorflow/docs) như:\n",
        "\n",
        "+ `tfdocs.modeling.EpochDots`: để tránh việc in quá nhiều log nếu training nhiều epoch.\n",
        "+ `tfdocs.plots.HistoryPlotter`: để vẽ learning curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QnAtAjqRYVXe",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/tensorflow/docs\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.modeling\n",
        "import tensorflow_docs.plots"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcFAS4uhjcXY",
        "colab_type": "text"
      },
      "source": [
        "**Khai báo thư mục để chứa tensorboard log:**\n",
        "\n",
        "+ `tempfile.mkdtemp`: tạo một thư mục tạm\n",
        "+ `pathlib.Path`: cho phép tạo các đường dẫn theo cú pháp đơn giản\n",
        "+ `shutil.rmtree`: xóa một thư mục (để đảm bảo các log cũ sẽ bị xóa) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jj6I4dvTtbUe",
        "colab": {}
      },
      "source": [
        "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
        "shutil.rmtree(logdir, ignore_errors=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1cweoTiruj8O"
      },
      "source": [
        "\n",
        "### **2. Chuẩn bị data**\n",
        "\n",
        "Chúng ta sẽ sử dụng [Higgs Dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS) bao gồm 11&#x202F;000&#x202F;000 example, mỗi example có 28 feature, và được gán nhãn 0 hoặc 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gM0KVwTzVS3",
        "colab_type": "text"
      },
      "source": [
        "**Download data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YPjAvwb-6dFd",
        "colab": {}
      },
      "source": [
        "gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChbqPTXAzZia",
        "colab_type": "text"
      },
      "source": [
        "**Đọc data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AkiyUdaWIrww",
        "colab": {}
      },
      "source": [
        "FEATURES = 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SFggl9gYKKRJ"
      },
      "source": [
        "Sử dụng `tf.data.experimental.CsvDataset` để đọc dữ liệu trực tiếp từ tệp csv ở dạng nén."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QHz4sLVQEVIU",
        "colab": {}
      },
      "source": [
        "ds = tf.data.experimental.CsvDataset(gz, [float(),]*(FEATURES+1), compression_type=\"GZIP\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cNYstH0zfEF",
        "colab_type": "text"
      },
      "source": [
        "**Tái cấu trúc dữ liệu của mỗi example:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HzahEELTKlSV"
      },
      "source": [
        "Mỗi phần tử của `ds` là một list gộp chung cả label và feature. Ta cần định dạng lại cấu trúc dữ liệu sao cho mỗi phần tử là một tuple của *features* và *label*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zPD6ICDlF6Wf",
        "colab": {}
      },
      "source": [
        "def pack_row(*row):\n",
        "  label = row[0]\n",
        "  features = tf.stack(row[1:],1)\n",
        "  return features, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4oa8tLuwLsbO"
      },
      "source": [
        "Tiếp theo sử dụng hàm `map` để áp dụng hàm `pack_row` lên tất cả các example. \n",
        "\n",
        "Đầu tiên, ta tạo batch với kích thước mỗi batch là 10000. Trong quá trình tạo batch, áp dụng hàm `pack_row` lên từng example trong batch. Sau đó, unbatch lại như lúc đầu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-w-VHTwwGVoZ",
        "colab": {}
      },
      "source": [
        "packed_ds = ds.batch(10000).map(pack_row).unbatch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lUbxc5bxNSXV"
      },
      "source": [
        "Note: Cách áp dụng kỹ thuật *data agumentation* là hoàn toàn tương tự. Ta định nghĩa một hàm augmentation. Sau đó dùng hàm `map` để áp dụng lên tất cả các phần tử trong dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8XFVcvh0GOa",
        "colab_type": "text"
      },
      "source": [
        "**Explore data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TfcXuv33Fvka",
        "colab": {}
      },
      "source": [
        "for features,label in packed_ds.batch(1000).take(1):\n",
        "  print(features[0])\n",
        "  plt.hist(features.numpy().flatten(), bins = 101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ICKZRY7gN-QM"
      },
      "source": [
        "Sử dụng 1000 example đầu tiên cho validation và 10 000 example còn lại cho training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hmk49OqZIFZP",
        "colab": {}
      },
      "source": [
        "N_VALIDATION = int(1e3)\n",
        "N_TRAIN = int(1e4)\n",
        "BUFFER_SIZE = int(1e4)\n",
        "BATCH_SIZE = 500\n",
        "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FP3M9DmvON32"
      },
      "source": [
        "Chúng ta sử dụng hàm `Dataset.skip` và `Dataset.take` để phân chia tập train và tập validation. Sử dụng `Dataset.cache` để đảm bảo không cần đọc lại dữ liệu từ file mỗi epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H8H_ZzpBOOk-",
        "colab": {}
      },
      "source": [
        "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
        "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6PMliHoVO3OL"
      },
      "source": [
        "**Tạo batch:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y7I4J355O223",
        "colab": {}
      },
      "source": [
        "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lglk41MwvU5o"
      },
      "source": [
        "### **3. Ảnh hưởng của capacity đến overfitting**\n",
        "\n",
        "Cách đơn giản nhất để tránh overfitting là bắt đầu với một model đơn giản: một model với ít parameter (được xác định bởi số layer và số unit của mỗi layer). Thuật ngữ tiếng anh để chỉ số lượng parameter của model là capacity (*từ gốc là network's capacity hoặc model's capacity*).\n",
        "\n",
        "Về mặt lý thuyết, một model với nhiều parameter sẽ có khả năng ghi nhớ tốt hơn. Nó sẽ dễ dàng học ánh xạ $x \\rightarrow y$ (hay $features \\rightarrow label$) mà thiếu đi sự tổng quát hóa. Điều này dẫn tới kết quả không tốt khi model gặp các dữ liệu không có trong tập training.\n",
        "\n",
        "Tuy nhiên, không có một công thức để xác định capacity thích hợp. Chúng ta phải tiến hành nhiều thí nghiệm để lựa chọn: đầu tiên bắt đầu với model có capacity nhỏ sau đó tăng dần cho tới bắt gặp hiện tượng overfitting bằng cách quan sát validation loss.\n",
        "\n",
        "Trong phần này, chúng ta sẽ so sánh 4 model với capacity tăng dần: **Tiny mode**, **Smal model**, **Medium model** và **Large model**. Chúng ta cũng sẽ sử dụng cùng một *training strategy* cho cả 4 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ReKHdC2EgVu"
      },
      "source": [
        "**Khai báo training stategy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pNzkSkkXSP5l"
      },
      "source": [
        "*(1) Sử dụng Optimizer và Learning Rate giống nhau*\n",
        "\n",
        "Các model thường cho accuracy tốt hơn nếu learning rate được giảm từ từ trong quá trình training. Kỹ thuật này được gọi là Adaptive Learning Rate.\n",
        "Chúng ta sẽ sử dụng InverseTimeDecay từ `optimizers.schedules`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LwQp-ERhAD6F",
        "colab": {}
      },
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "  0.001,\n",
        "  decay_steps=STEPS_PER_EPOCH*1000,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "\n",
        "def get_optimizer():\n",
        "  return tf.keras.optimizers.Adam(lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kANLx6OYTQ8B"
      },
      "source": [
        "`InverseTimeDecay` có công thức như sau:\n",
        "\n",
        "$$decayed\\_learning\\_rate = learning\\_rate / (1 + decay\\_rate * t)$$\n",
        "\n",
        "Cụ thể, chúng ta đã sử dụng: $learning\\_rate=0.001$, $decay\\_rate=1$. Như vậy, `learning rate` sẽ giảm đi $1/2$ sau 1000 epoch và $1/3$ sau 2000 epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lw--6cd8HU4",
        "colab_type": "text"
      },
      "source": [
        "Đoạn code sau sẽ vẽ một đồ thị minh họa sự thay đổi của `learning rate`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HIo_yPjEAFgn",
        "colab": {}
      },
      "source": [
        "step = np.linspace(0, 1e5)\n",
        "lr = lr_schedule(step)\n",
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ya7x7gr9UjU0"
      },
      "source": [
        "*(2) Sử dụng chung các callback*\n",
        "\n",
        "+ Để tránh hiển thị quá nhiều log khi train model trong nhiều epoch, ta sử dụng `tfdocs.EpochDots`. Nó sẽ chỉ in ra log sau mỗi 100 epoch.\n",
        "\n",
        "+ Sử dụng `callbacks.EarlyStopping` để tránh việc training lâu không cần thiết. Hàm callback này sẽ theo dõi `val_binary_crossentropy`. Nếu đại lượng này không giảm trong liên tiếp 200 epoch, quá trình training sẽ dừng lại. \n",
        "\n",
        "+ Sử dụng `callbacks.TensorBoard` để theo dõi log trong quá trình training. Một công cụ rất hiệu quả để đánh giá overfitting và underfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vSv8rfw_T85n",
        "colab": {}
      },
      "source": [
        "def get_callbacks(name):\n",
        "  return [\n",
        "    tfdocs.modeling.EpochDots(),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
        "    tf.keras.callbacks.TensorBoard(logdir/name),\n",
        "  ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VhctzKhBWVDD"
      },
      "source": [
        "*(3) Sử dụng cùng thiết lập cho `Model.compile` và `Model.fit`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xRCGwU3YH5sT",
        "colab": {}
      },
      "source": [
        "def compile_and_fit(model, name, optimizer=None, max_epochs=10000):\n",
        "  if optimizer is None:\n",
        "    optimizer = get_optimizer()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                metrics=[\n",
        "                  tf.keras.losses.BinaryCrossentropy(\n",
        "                      from_logits=True, name='binary_crossentropy'),\n",
        "                  'accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "    train_ds,\n",
        "    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "    epochs=max_epochs,\n",
        "    validation_data=validate_ds,\n",
        "    callbacks=get_callbacks(name),\n",
        "    verbose=0)\n",
        "  return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EuLRTro9WyO",
        "colab_type": "text"
      },
      "source": [
        "**Khai báo `size_histories` để lưu tất cả các history:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X72IUdWYipIS",
        "colab": {}
      },
      "source": [
        "size_histories = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mxBeiLUiWHJV"
      },
      "source": [
        "#### **3.1. Tiny model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a6JDv12scLTI"
      },
      "source": [
        "Sử dụng 1 hidden layer với 16 unit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EZh-QFjKHb70",
        "colab": {}
      },
      "source": [
        "tiny_model = tf.keras.Sequential([\n",
        "    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bdOcJtPGHhJ5",
        "colab": {}
      },
      "source": [
        "size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rS_QGT6icwdI"
      },
      "source": [
        "**Vẽ learning curve:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dkEvb2x5XsjE",
        "colab": {}
      },
      "source": [
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LGxGzh_FWOJ8"
      },
      "source": [
        "#### **3.2. Small model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YjMb6E72f2pN"
      },
      "source": [
        "Sử dụng 2 hidden layer với 16 unit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QKgdXPx9usBa",
        "colab": {}
      },
      "source": [
        "small_model = tf.keras.Sequential([\n",
        "    # `input_shape` is only required here so that `.summary` works.\n",
        "    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(16, activation='elu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LqG3MXF5xSjR",
        "colab": {}
      },
      "source": [
        "size_histories['Small'] = compile_and_fit(small_model, 'sizes/Small')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L-DGRBbGxI6G"
      },
      "source": [
        "#### **3.3. Medium model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SrfoVQheYSO5"
      },
      "source": [
        "Sử dụng 3 hidden layer với 64 unit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jksi-XtaxDAh",
        "colab": {}
      },
      "source": [
        "medium_model = tf.keras.Sequential([\n",
        "    layers.Dense(64, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(64, activation='elu'),\n",
        "    layers.Dense(64, activation='elu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ofn1AwDhx-Fe",
        "colab": {}
      },
      "source": [
        "size_histories['Medium']  = compile_and_fit(medium_model, \"sizes/Medium\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vIPuf23FFaVn"
      },
      "source": [
        "#### **3.4. Large model**\n",
        "\n",
        "Sử dụng 4 hidden layer với 512 unit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ghQwwqwqvQM9",
        "colab": {}
      },
      "source": [
        "large_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U1A99dhqvepf",
        "colab": {}
      },
      "source": [
        "size_histories['large'] = compile_and_fit(large_model, \"sizes/Large\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fy3CMUZpzH3d"
      },
      "source": [
        "### **4. Đánh giá overfitting từ learning curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0XmKDtOWzOpk",
        "colab": {}
      },
      "source": [
        "plotter.plot(size_histories)\n",
        "a = plt.xscale('log')\n",
        "plt.xlim([5, max(plt.xlim())])\n",
        "plt.ylim([0.5, 0.7])\n",
        "plt.xlabel(\"Epochs [Log Scale]\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HSlo1F4xHuuM"
      },
      "source": [
        "Đường nét liền là training loss, nét đứt là validation loss. Các màu khác nhau đại diện cho các model với các capacity khác nhau.\n",
        "\n",
        "Nhìn vào learning curve, ta thấy model có capacity càng lớn thì training loss càng nhỏ và càng nhanh overfitting (validation loss càng nhanh đạt cực tiểu và sau đó tăng)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DEQNKadHA0M3"
      },
      "source": [
        "### **5. Xem learning curve bằng TensorBoard**\n",
        "\n",
        "Chúng ta đã quen với việc lưu lại các metric như loss, accuracy, v.v. trong training. Sau khi kết thức training, dùng `matplotlib` để vẽ learning curve. Tuy nhiên, các này có nhược điểm là ta không thể đánh giá model ngay trong quá trình training.\n",
        "\n",
        "`Tensorboard` là một công cụ giải quyết vấn đề này. Trong quá trình training, Tensorboard Callback được gọi định kỳ (sau một số step nào đó), nó sẽ lưu các metric vào các file log trong một thư mục do chúng ta chỉ định. Ta có thể đọc các file này ngay trong quá trình training và vẽ các learning curve để theo dõi sự thay đổi của các metric.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER15_Wd-Hruk",
        "colab_type": "text"
      },
      "source": [
        "**Để mở tensorboard trong notebook:**\n",
        "\n",
        "Tham số `logdir` cho biết thư mục chứa các file log."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seZgZaZlCM8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {logdir}/sizes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RDQDBKYZBXF_"
      },
      "source": [
        "**Chia sẻ tensorboard lên [TensorBoard.dev](https://tensorboard.dev/)**:\n",
        "\n",
        "Chúng ta cũng có thể chia sẻ tensorboard với mọi người bằng cách upload các file log lên TensorBoard.dev."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM4j5F8ECtg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard dev upload --logdir  {logdir}/sizes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a_6v2LLSJb",
        "colab_type": "text"
      },
      "source": [
        "Lưu ý: Lệnh này yêu cầu xác thực tài khoản Google."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fjqx3bywDPjf"
      },
      "source": [
        "**Xem một tensoboard được chia sẻ lên [TensorBoard.dev](https://tensorboard.dev/)**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dX5fcgrADwym",
        "colab": {}
      },
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\",\n",
        "    width=\"100%\", height=\"800px\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ASdv7nsgEFhx"
      },
      "source": [
        "## **6. Áp dụng phương pháp tránh overfitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YN512ksslaxJ"
      },
      "source": [
        "Chúng ta sẽ sao lưu các log của Tiny model để so sánh với các kết quả khi áp dụng phương pháp tránh overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "40k1eBtnQzNo",
        "colab": {}
      },
      "source": [
        "shutil.rmtree(logdir/'regularizers/Tiny', ignore_errors=True)\n",
        "shutil.copytree(logdir/'sizes/Tiny', logdir/'regularizers/Tiny')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vFWMeFo7jLpN",
        "colab": {}
      },
      "source": [
        "regularizer_histories = {}\n",
        "regularizer_histories['Tiny'] = size_histories['Tiny']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4rHoVWcswFLa"
      },
      "source": [
        "### **6.1. Sử dụng Weight Regularization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGdwuHdnMZtV",
        "colab_type": "text"
      },
      "source": [
        "Một trong các cách tránh overfitting là sử dụng weight regularization. Có hai loại regularization phổ biến là:\n",
        "\n",
        "* [L1 regularization](https://developers.google.com/machine-learning/glossary/#L1_regularization):\n",
        "\n",
        "$$\\theta^* = \\arg min _ \\theta \\left( loss + \\lambda \\sum _ i |\\theta _ i| \\right)$$\n",
        "\n",
        "* [L2 regularization](https://developers.google.com/machine-learning/glossary/#L2_regularization):\n",
        "\n",
        "$$\\theta^* = \\arg min _ \\theta \\left( loss + \\lambda \\sum _ i \\theta _ i ^ 2 \\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kRxWepNawbBK"
      },
      "source": [
        "Trong thực tế L2-regularization được sử dụng nhiều hơn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFGmcwduwVyQ",
        "colab": {}
      },
      "source": [
        "l2_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001),\n",
        "                 input_shape=(FEATURES,)),\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "regularizer_histories['l2'] = compile_and_fit(l2_model, \"regularizers/l2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bUUHoXb7w-_C"
      },
      "source": [
        "`l2(0.001)` tức $\\lambda = 0.001$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7wkfLyxBZdh_",
        "colab": {}
      },
      "source": [
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kx1YHMsVxWjP"
      },
      "source": [
        "Từ đồ thị trên ta thấy L2-regularization giúp giảm vấn đề overfitting trên Large model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLLCN1NmQ7NY",
        "colab_type": "text"
      },
      "source": [
        "### **6.2. Sử dụng Dropout**\n",
        "\n",
        "Dropout là một kỹ thuật tránh overfitting được sử dụng nhiều nhất được đề xuất bởi Hinton và các sinh viên tại đại học Toronto.\n",
        "\n",
        "Ý tưởng của Dropout là ngẫu nhiên đóng, mở các node trong quá trình training. Điều này khiến network không thể chỉ dựa vào một vài node để cho ra kết quả, thay vào đó mỗi node phải học một feature có ý nghĩa.\n",
        "\n",
        "Lưu ý: Ví dụ dưới đây, ta thêm vào network các dropout layer. Tuy nhiên, chúng chỉ được dùng trong quá trình training. Trong testing, các dropout layer sẽ không có tác dụng."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFEYvtrHxSWS",
        "colab": {}
      },
      "source": [
        "dropout_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "regularizer_histories['dropout'] = compile_and_fit(dropout_model, \"regularizers/dropout\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SPZqwVchx5xp",
        "colab": {}
      },
      "source": [
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4zlHr4iaI1U6"
      },
      "source": [
        "Cả L2 regularization và Dropout giúp cải thiện chất lượng của Large model. Tuy nhiên kết quả vẫn còn hạn chế.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u7qMg_7Nwy5t"
      },
      "source": [
        "### **6.3. Kết hợp L2 và dropout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7zfs_qQIw1cz",
        "colab": {}
      },
      "source": [
        "combined_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "regularizer_histories['combined'] = compile_and_fit(combined_model, \"regularizers/combined\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qDqBBxfI0Yd8",
        "colab": {}
      },
      "source": [
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tE0OoNCQNTJv"
      },
      "source": [
        "Việc kết hợp các phương pháp tránh overfitting thông thường sẽ hiệu quả nhất."
      ]
    }
  ]
}